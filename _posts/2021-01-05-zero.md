---
layout: post
title:  "[Distributed ML] Memory Optimization with ZeRO"
date:   2020-01-01 01:22:24 -0800
categories: distributed_ml
---

I first came across the [ZeRO paper](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/) as part of Microsoft's announcement that they were able to train models over 100B parameters. The things that I found exciting about this paper is that instead of scaling out using more machines, these optimizations attempt to use fewer resources by training more efficiently.

ZeRO(Zero Redundancy Optimizer) optimizations during model training are able to achieve the following:

1. Train large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. 
2. 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, 
ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply.
3. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the worldâ€™s largest language model (17B parameters) with record breaking accuracy.

The key takeaways of the paper are the following:

1. Without model parallelism, you can scale to 17B parameters using smart memory optimization techniques.
2. ZeRO is easy to use and does not have sharding challenges like Model Parallelism that researchers have to deal with.
3. ZeRO does not attempt to solve a DataParalelism or Model Parallelism problem specifically. Data Parallelism enables  communication and computation efficiency at the cost of memory efficiency. Model Parallelism enables memory efficiency at the cost of computation and communication efficiency. ZeRO attempts to enable memory efficiency without foregoing communication and computation efficiency. 

The authors of the paper analyze the memory consumption of model training workflow and arrive at the following conclusion. The memory consumed by model training can be divided into two main buckets:

1. model states (e.g optimizer states, gradients, model parameters)
Some optimizers e.g Adam store momentum and variance of each of the gradients. This adds to the existing requirement of storing gradients and model parameters. During FP16 training, the forward and backward pass require FP16 weights and activations but when we update the model parameters, an FP32 copy of the weights as well as all other optimizer states are kept. For example, GPT-2 with 1.5B parameters requires 24GB  which is much higher than 3GB required for FP16 parameters.
2. residual states (e.g activations, temp buffers, fragmented memory)
The authors find that activations can take a large amount of memory and even with activation checkpointing or recomputation the size of memory can be large. For example, GPT-2 with a batch size of 32 requires 60GB which is a significant amount. The other residual state are large buffers allocated to tensors for allreduce communication. These large buffers are used to fuse gradients into a single flattened buffer. These flattened FP32 buffers can be 6GB of memory for a 1.5B parameter model. Finally given that tensors are allocated and deallocated when training large models, it is possible to run out of contiguous memory even if you have aggregated memory available. 


ZeRO shards the above memory consumption across all devices instead of replicating them like Data Parallelism. The authors use key insights that relate to high computational granularity of Data Parallism, low memory utilization of model parallelism and the fact that all parameters of the model are not needed at a layer granularity. ZeRO attempts to reduce the per device memory footprint of a model linearly with the number of devices while keeping the communication cost as low as possible(close to the data parallelism baseline).

To deal with the first kind of memory consumption i.e model states, there are three main stages of ZeRo that can be enabled:
1. Zero-1: Optimizer state partitioning which results in a 4x memory reduction with the same communication volume as Data Parallelism. The optimizer state is sharded across the N data parallel processes such that each process is only responsible for updating 1/N th of the optimizer state. An allgather is performed at the end of each training step to get the latest parameters on each of the processes.
2. Zero-2: Gradient partitioning: 8x memory reduction with the same communication volume as Data Parallelism. Similar to ZeRO-1, gradients are reduced on the process that is reponsible for updating the model parameters given the sharded optimizer state. This is essentially a reduce-scatter operations where the gradients are reduced to different processes.
3. ZeRO-3: Parameter partitioning: Nx memory reduction where N=number of GPUs but results in a 50x communication volume increase. Model parameters are partitioned along similar lines of 1. and 2. Each process stores parameters corresponding to it's partition. Parameters from a given process are broadcast to all other processes as needed for the forward and backward pass.

![Per Device Memory Savings](/assets/images/zero/zero-per-device-usage.png)

For optimizing memory consumption in the second bucket i.e residual state memory, ZeRO employs the following methods:
1. Activation state - Activation layers are partitioned across all processes and are replicated across all processes using allgather right before they are required for computation (e.g backward pass). This is used in conjunction with activation checkpointing such that we are checkpointing the sharded activations. CPU offload technique can also be used to reduce the device memory footprint.
2. Constant size buffers - ZeRO uses a constant size large buffer size so that the model size does not influence the buffer size allocated. Buffer size increases with model size which means that a 3B parameter model and 32-bit fused buffer will need 12GB of memory. These bufferes are typically used for fusing gradients or tensors before allreduce operations. The large the size of the buffer, the higher the bandwidth (hence more efficient) used.
3. memory defragmentation - ZeRO uses preallocated contiguous memory for activation checkpoints and gradients based on the lifetime of tensors. Interleaving long term and short term memory results in OOM since contiguous memory may not be available even if net memory is present. The other inefficiency is related to delay in finding a contiguous memory reference.

The authors also analyze the communication volume of using the above strategies for ZeRO-M and ZeRO-R. When using ZeRO-1 and ZeRO-2 the communication throughput remains the same as baseline DDP. However for ZeRO-3, there is a 1.5x increase the communication throughput due to an extra allgather of parameters at the beginning of every layer computation. For ZeRO-R, activation partitioning with Model parallelism results in a decrease in memory consumption by the model parallelism degress. This means batch size can be increased by the same degree. CPU offload means 0 memory consumption due to activation checkpointing. Activation checkpointing with CPU offload should be used when the overhead of Data parallel communication is higher than the CPU-device link overhead.

Zero+MP can still be used if you are seeing a degradation in accuracy when you increase the batch size to very large numbers. Large models can still benefit from MP when used in conjunction with zero
There is a 8x increase in model size(170B > 40B), 10x increase in speed, linear scalability 64->400 GPUs and 1.3B is the most that PyTorch DDP can support.
ZeRO-100B is a an implementation which has only ZeRO-1 and ZeRO-2 enabled. The allows researchers to train upto 13B parameter models without Model Parallelism or Parameter partitioning. 

The authors present results carried out on ZeRO-100B which implements ZeRO-1, ZeRO-2 and ZeRO-R. Using this implementation a 170B parameter model can be trained(8x larger and 10x faster at that point in time as compared to SOTA).

Key results:

![ZeRO MP Performance](/assets/images/zero/zero-mp-perf.png)

ZeRO-100B with MP achieves 15PFlops or over 30% of the peak when compared with the Model parallelism baseline (Megatron-LM). This is for models between 8B-100B. Beyond 100B there is a performance drop due to insufficient GPUs. Increasing the number of nodes will reduce the memory consumption and increase efficiency.

![Zero Scalability](/assets/images/zero/zero-scalability.png)

ZeRO-100B demonstrates linear scalability for large model sizes. For example, the figure shown almost linear scalability for a 60B model going from 64-400GPUs

![ZeRO Model Throughput](/assets/images/zero/zero-model-throughput.png)

ZeRO-100B can train a 13B parameter model on 128 GPUs without model parallelism. Without ZeRO Data parallelism can train a 1.4B parameter model with an efficiency of less than 30TFlops. ZeRO also enabled training on lower-end compute nodes that don't have NVLinks or NVSwithces that are required for efficient Model Parallelism.

![ZeRO Vs Megatron](/assets/images/zero/zero-vs-megatron.png)]

ZeRO-100B is able to train Turing-NLG 17B parameter model and achieves a lower perplexity than the previous SOTA model Megatron-LM of 8.3B parameters.